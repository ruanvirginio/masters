{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJDwuasZi9i2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "url_hourly = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/transformers_dataset.csv\"\n",
        "df_hourly = pd.read_csv(url_hourly,  sep=';', encoding='latin-1')\n",
        "\n",
        "url_daily = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/daily_peak_transformers_dataset.csv\"\n",
        "df_daily = pd.read_csv(url_daily,  sep=';', encoding='latin-1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# df_count = df_daily.groupby('id').count()\n",
        "\n",
        "# df_count = df_count.sort_values('datahora').tail(41).reset_index() # esses são os trafos com mais de 97,5% de linhas preenchidas\n",
        "# trafos_escolhidos = df_count['id'].unique().tolist()\n",
        "# df_filtrado = df_daily[df_daily['id'].isin(trafos_escolhidos)]\n",
        "\n",
        "# fig_aparente = px.line(df_filtrado, x='datahora', y='S', color='id',\n",
        "#                        title='Potência Aparente ao Longo do Tempo por Transformador',\n",
        "#                        labels={'S': 'Potência Aparente (kVA)', 'Dia': 'Data'})\n",
        "\n",
        "# fig_aparente.show()\n",
        "# fig_aparente.write_html(\"Demanda ao longo do tempo - IQR.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "# Lista de transformadores e modelos\n",
        "trafos = ['T1', 'T2']\n",
        "modelos = ['SVR', 'XGB', 'LGBM', 'GBR', 'LSTM', 'CNN', 'CNN_LSTM']\n",
        "\n",
        "# Diretório base onde estão os modelos\n",
        "base_dir = r\"G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\modelos\"\n",
        "\n",
        "# Dicionário para armazenar os modelos carregados\n",
        "modelos_carregados = {}\n",
        "\n",
        "for trafo in trafos:\n",
        "    for modelo in modelos:\n",
        "        # Define a extensão correta\n",
        "        extensao = \"h5\" if modelo in [\"LSTM\", \"CNN\", \"CNN_LSTM\"] else \"pkl\"\n",
        "        caminho = os.path.join(base_dir, f\"{modelo}_{trafo}.{extensao}\")\n",
        "        \n",
        "        # Inicializa a chave no dicionário\n",
        "        modelos_carregados[(modelo, trafo)] = None\n",
        "        \n",
        "        # Carrega apenas se o arquivo existir\n",
        "        if os.path.exists(caminho):\n",
        "            if modelo in [\"LSTM\", \"CNN\", \"CNN_LSTM\"]:\n",
        "                modelos_carregados[(modelo, trafo)] = load_model(caminho)\n",
        "            else:\n",
        "                modelos_carregados[(modelo, trafo)] = joblib.load(caminho)\n",
        "            print(f\"✅ Modelo carregado: {caminho}\")\n",
        "        else:\n",
        "            print(f\"⚠️ Arquivo não encontrado, ignorando: {caminho}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos analisar seus dados originais para entender a estrutura\n",
        "print(\"=== ANALISANDO DADOS ORIGINAIS ===\")\n",
        "print(f\"Shape do df_daily: {df_daily.shape}\")\n",
        "print(f\"Colunas: {df_daily.columns.tolist()}\")\n",
        "print(f\"Tipos de dados:\\n{df_daily.dtypes}\")\n",
        "\n",
        "# Ver um exemplo dos dados do T1\n",
        "print(\"\\n=== DADOS DO TRAFO T1 ===\")\n",
        "df_t1 = df_daily[df_daily['id'] == 'T1'].copy()\n",
        "print(f\"Quantidade de dados T1: {len(df_t1)}\")\n",
        "print(f\"Período: {df_t1['datahora'].min()} até {df_t1['datahora'].max()}\")\n",
        "print(f\"Primeiras linhas T1:\")\n",
        "print(df_t1.head())\n",
        "\n",
        "# Ver a distribuição da variável target 'S'\n",
        "print(f\"\\nEstatísticas da Potência Aparente (S):\")\n",
        "print(df_t1['S'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TREINNAMENTO ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# url_hourly = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/transformers_dataset.csv\"\n",
        "# df_hourly = pd.read_csv(url_hourly,  sep=';', encoding='latin-1')\n",
        "\n",
        "url_daily = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/daily_peak_transformers_dataset.csv\"\n",
        "df_daily = pd.read_csv(url_daily,  sep=';', encoding='latin-1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# IMPORTAÇÕES E CONFIGURAÇÕES\n",
        "# ========================================================================\n",
        "# % Bibliotecas padrão para manipulação de dados, plots e utilitários\n",
        "# & np/pandas/matplotlib/plotly para análise e visualização\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'browser'\n",
        "\n",
        "# % Métricas e busca de hiperparâmetros\n",
        "# & sklearn para modelos tradicionais e validação temporal\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import joblib\n",
        "\n",
        "# % Keras / TensorFlow para redes neurais (LSTM, CNN)\n",
        "# & keras_tuner para otimização das redes (usada uma vez por trafo/modelo)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras_tuner as kt\n",
        "\n",
        "# % Garantias de reprodutibilidade (seeds)\n",
        "# & define seeds para numpy, tensorflow e random\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # & para evitar backend interativo em servidores\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÕES AUXILIARES (PLOTS, MÉTRICAS, EXPORTAÇÃO)\n",
        "# ========================================================================\n",
        "def gerar_tabela_metricas_por_fold(trafo, modelo, fold_rmse, fold_mae):\n",
        "    # % Gera um DataFrame com RMSE/MAE de cada fold para exibir e salvar\n",
        "    # & retorno fácil de imprimir / salvar\n",
        "    return pd.DataFrame({\n",
        "        'Fold': [f'Fold {i+1}' for i in range(len(fold_rmse))],\n",
        "        'Trafo': trafo,\n",
        "        'Modelo': modelo,\n",
        "        'RMSE': np.round(fold_rmse, 4),\n",
        "        'MAE': np.round(fold_mae, 4)\n",
        "    })\n",
        "\n",
        "def plotar_ultimo_fold(datas, y_real, y_pred, trafo, modelo, freq):\n",
        "    # % Salva gráfico do último fold como PDF (útil para relatório)\n",
        "    # & fecha a figura ao final para não consumir memória em loop grande\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(datas, y_real, label='Real', color='blue')\n",
        "    plt.plot(datas, y_pred, label=f'Previsto ({modelo})', linestyle='--', color='orange')\n",
        "    plt.xlabel('Dia' if freq=='daily' else 'Hora')\n",
        "    plt.ylabel('Potência Aparente (kVA)')\n",
        "    plt.title(f'Previsão Último Fold - {modelo} ({trafo})')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "    plt.savefig(f'plots/PLOT_{modelo}_{trafo}_{freq}_ultimo_fold.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plotar_todos_folds(lista_datas, lista_reais, lista_previstos, trafo, modelo, eixo_label='Data'):\n",
        "    # % Plota todos os folds concatenados em uma figura interativa (Plotly)\n",
        "    # & útil para inspecionar continuidade e erros em diferentes períodos\n",
        "    if len(lista_datas) == 0:\n",
        "        return\n",
        "    datas_todas = pd.to_datetime(np.concatenate(lista_datas))\n",
        "    reais_todos = np.concatenate(lista_reais)\n",
        "    previstos_todos = np.concatenate(lista_previstos)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=datas_todas, y=reais_todos, mode='lines', name='Real'))\n",
        "    fig.add_trace(go.Scatter(x=datas_todas, y=previstos_todos, mode='lines', name=f'Previsto ({modelo})', line=dict(dash='dash')))\n",
        "    fig.update_layout(title=f'Previsão em Todos os Folds - {trafo} ({modelo})',\n",
        "                      xaxis_title=eixo_label, yaxis_title='Potência Aparente', hovermode='x unified')\n",
        "    fig.show()\n",
        "\n",
        "# ========================================================================\n",
        "# GRIDS BASE PARA HYPERPARAMS (AJUSTÁVEIS)\n",
        "# ========================================================================\n",
        "param_grids_base = {\n",
        "    'SVR': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "        'epsilon': [0.01, 0.1, 0.5]\n",
        "    },\n",
        "    'RFR': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [5, 10, 15],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'GBR': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [3, 5]\n",
        "    },\n",
        "    'XGB': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [3, 5]\n",
        "    },\n",
        "    'LGBM': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_param_grids_por_freq(freq, modelo):\n",
        "    # % Ajusta o grid padrão de acordo com a granularidade dos dados\n",
        "    # & reduções para dados horários (economia de tempo)\n",
        "    base_params = param_grids_base[modelo].copy()\n",
        "    if freq == 'hourly':\n",
        "        if modelo == 'SVR':\n",
        "            base_params['C'] = [1, 10]\n",
        "            base_params['gamma'] = ['scale', 0.01]\n",
        "            base_params['epsilon'] = [0.1]\n",
        "        elif modelo in ['RFR', 'GBR', 'XGB', 'LGBM']:\n",
        "            base_params['n_estimators'] = [50, 100]\n",
        "    return base_params\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÃO AUXILIAR: CRIA MODELO KERAS A PARTIR DE HYPERPARAMS FIXOS\n",
        "# ========================================================================\n",
        "def build_keras_from_hps_fixed(modelo, hps):\n",
        "    # % Recebe o nome do modelo (LSTM/CNN/CNN_LSTM) e um dict-like de hyperparams\n",
        "    # & Retorna um modelo Keras já compilado pronto para treinar\n",
        "    model = Sequential()\n",
        "    if modelo == 'LSTM':\n",
        "        # % Para LSTM pegamos 'units1' (número de neurônios) do best_hps\n",
        "        model.add(LSTM(units=int(hps.get('units1', 20)), input_shape=(None, 1)))\n",
        "        model.add(Dense(1))\n",
        "    elif modelo == 'CNN':\n",
        "        # % CNN simples 1D: filters e kernel são fixos nos valores do tuner\n",
        "        model.add(Conv1D(filters=int(hps.get('filters', 32)), kernel_size=2, activation='relu', input_shape=(None, 1)))\n",
        "        model.add(MaxPooling1D(2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "    elif modelo == 'CNN_LSTM':\n",
        "        # % CNN + LSTM: primeiro convolucional, depois LSTM\n",
        "        model.add(Conv1D(filters=int(hps.get('filters', 32)), kernel_size=2, activation='relu', input_shape=(None, 1)))\n",
        "        model.add(MaxPooling1D(2))\n",
        "        model.add(LSTM(units=int(hps.get('units1', 20))))\n",
        "        model.add(Dense(1))\n",
        "    else:\n",
        "        raise ValueError(f\"Modelo Keras desconhecido: {modelo}\")\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÃO PRINCIPAL: OTIMIZAÇÃO UMA-VEZ + TREINO POR FOLD COM PARAMS FIXOS\n",
        "# ========================================================================\n",
        "def treinar_e_prever_modelo_auto_sem_data_leakage(\n",
        "    data, trafos_escolhidos, modelo, janela=None, epochs=15, batch_size=32, n_iter_search=5\n",
        "):\n",
        "    \"\"\"\n",
        "    % Função principal que:\n",
        "    % 1) Detecta frequência (diária vs. horária)\n",
        "    % 2) Para cada trafo:\n",
        "    %    a) Prepara janelas (sliding window)\n",
        "    %    b) Executa busca de hiperparâmetros APENAS UMA VEZ no conjunto inicial (tuning set)\n",
        "    %    c) Usa os hiperparâmetros encontrados para treinar/avaliar cada fold (TimeSeriesSplit)\n",
        "    %    d) Salva modelos, métricas e parâmetros\n",
        "    &\n",
        "    & Retorna: (resultados_df, parametros_df)\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    parametros_finais = []\n",
        "    os.makedirs('modelos', exist_ok=True)\n",
        "    data = data.copy()\n",
        "    data['datahora'] = pd.to_datetime(data['datahora'])\n",
        "\n",
        "    # % Detecta granularidade pela mediana do delta entre timestamps\n",
        "    delta = data['datahora'].diff().median()\n",
        "    freq = 'daily' if delta >= pd.Timedelta('1D') else 'hourly'\n",
        "    if janela is None:\n",
        "        janela = 30 if freq == 'daily' else 24*3\n",
        "\n",
        "    # & Percorre cada trafo solicitado\n",
        "    for trafo in trafos_escolhidos:\n",
        "        print(f\"\\n🔹 Treinando {modelo} para {trafo} ({freq})...\")\n",
        "\n",
        "        # % Prepara os dados do trafo (ordenados e sem NA)\n",
        "        df_trafo = data[data['id'] == trafo][['datahora', 'S']].sort_values('datahora').dropna()\n",
        "        S_values = df_trafo['S'].values\n",
        "        datas = df_trafo['datahora'].values\n",
        "\n",
        "        n_samples = len(S_values) - janela\n",
        "        if n_samples <= 0:\n",
        "            print(f\"⚠️ Dados insuficientes para {trafo}. Pulando...\")\n",
        "            continue\n",
        "\n",
        "        # % Cria janelas deslizantes (sliding window) de forma eficiente\n",
        "        # & Para X: cada amostra é uma janela de tamanho 'janela'; y é o valor seguinte\n",
        "        X = np.lib.stride_tricks.sliding_window_view(S_values, janela)[:n_samples]\n",
        "        y = S_values[janela:janela + n_samples]\n",
        "        datas_janela = datas[janela:janela + n_samples]\n",
        "\n",
        "        # % Ajuste de forma para modelos sklearn vs Keras\n",
        "        if modelo in ['LSTM', 'CNN', 'CNN_LSTM']:\n",
        "            # & shape = (n_samples, janela, 1)\n",
        "            X_keras = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "        else:\n",
        "            # & shape = (n_samples, janela)\n",
        "            X_sklearn = X.reshape((X.shape[0], -1))\n",
        "\n",
        "        # ============================\n",
        "        # ETAPA A: HYPERPARAM SEARCH (1x)\n",
        "        # ============================\n",
        "        if modelo in param_grids_base.keys():\n",
        "            # % Modelos scikit-learn: RandomizedSearchCV executado apenas uma vez\n",
        "            print(\"🔍 Executando RandomizedSearchCV apenas uma única vez (sem leakage)...\")\n",
        "            param_grid_freq = get_param_grids_por_freq(freq, modelo)\n",
        "\n",
        "            base_model = {\n",
        "                'SVR': SVR(),\n",
        "                'RFR': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "                'GBR': GradientBoostingRegressor(random_state=42),\n",
        "                'XGB': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
        "                'LGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
        "            }[modelo]\n",
        "\n",
        "            # % Define um conjunto inicial (tuning set) para busca de hiperparâmetros\n",
        "            # & usar os primeiros 70% dos exemplos evita usar informações do futuro\n",
        "            split_point = int(0.7 * len(X_sklearn))\n",
        "            X_tune, y_tune = X_sklearn[:split_point], y[:split_point]\n",
        "\n",
        "            # & RandomizedSearchCV com TimeSeriesSplit interno\n",
        "            search = RandomizedSearchCV(\n",
        "                base_model,\n",
        "                param_distributions=param_grid_freq,\n",
        "                n_iter=n_iter_search,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                cv=TimeSeriesSplit(n_splits=3),\n",
        "                n_jobs=-1 if modelo != 'SVR' else 1,\n",
        "                random_state=42,\n",
        "                verbose=0\n",
        "            )\n",
        "            search.fit(X_tune, y_tune)\n",
        "            best_params = search.best_params_\n",
        "            best_score = search.best_score_\n",
        "            print(f\"🎯 Melhores parâmetros encontrados (tuning): {best_params}\")\n",
        "            # % Registro do que foi encontrado\n",
        "            parametros_finais.append({\n",
        "                'Trafo': trafo,\n",
        "                'Modelo': modelo,\n",
        "                'Frequência': freq,\n",
        "                'Melhores_Parâmetros': best_params,\n",
        "                'Melhor_Score_Tuning': best_score,\n",
        "                'Metodo_Tuning': 'RandomizedSearchCV'\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            # % Redes neurais: usamos KerasTuner apenas uma vez no tuning set\n",
        "            print(\"🔍 Otimizando rede neural apenas uma vez com KerasTuner (sem leakage)...\")\n",
        "\n",
        "            # & Criador de modelo para o tuner (recebe hp)\n",
        "            def build_nn_tuner(hp):\n",
        "                model = Sequential()\n",
        "                if modelo == 'LSTM':\n",
        "                    units1 = hp.Int('units1', 20, 80, step=20)\n",
        "                    model.add(LSTM(units=units1, input_shape=(X_keras.shape[1], 1)))\n",
        "                    model.add(Dense(1))\n",
        "                elif modelo == 'CNN':\n",
        "                    filters = hp.Int('filters', 32, 64, step=32)\n",
        "                    model.add(Conv1D(filters=filters, kernel_size=2, activation='relu', input_shape=(X_keras.shape[1], 1)))\n",
        "                    model.add(MaxPooling1D(2))\n",
        "                    model.add(Flatten())\n",
        "                    model.add(Dense(1))\n",
        "                elif modelo == 'CNN_LSTM':\n",
        "                    filters = hp.Int('filters', 32, 64, step=32)\n",
        "                    units1 = hp.Int('units1', 20, 60, step=20)\n",
        "                    model.add(Conv1D(filters=filters, kernel_size=2, activation='relu', input_shape=(X_keras.shape[1], 1)))\n",
        "                    model.add(MaxPooling1D(2))\n",
        "                    model.add(LSTM(units=units1))\n",
        "                    model.add(Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mse')\n",
        "                return model\n",
        "\n",
        "            # & Prepara tuning set (70% inicial)\n",
        "            split_point = int(0.7 * len(X_keras))\n",
        "            X_tune, y_tune = X_keras[:split_point], y[:split_point]\n",
        "\n",
        "            # & Configura o tuner (Random search, poucas trials para economizar tempo)\n",
        "            tuner = kt.RandomSearch(\n",
        "                build_nn_tuner,\n",
        "                objective='val_loss',\n",
        "                max_trials=5,           # & ajuste: 5 para balancear custo/benefício\n",
        "                executions_per_trial=1,\n",
        "                overwrite=True,\n",
        "                directory='keras_tuner',\n",
        "                project_name=f'{modelo}_{trafo}_{freq}'\n",
        "            )\n",
        "\n",
        "            # & Early stopping usado na busca para acelerar e evitar overfitting\n",
        "            early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "            tuner.search(X_tune, y_tune,\n",
        "                         epochs=max(5, epochs//2),    # & usa menos epochs no tuner\n",
        "                         batch_size=batch_size,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[early_stop],\n",
        "                         verbose=0)\n",
        "\n",
        "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "            # % Convertendo os hyperparams do tuner para um dict plano para salvar\n",
        "            best_params = best_hps.values\n",
        "            parametros_finais.append({\n",
        "                'Trafo': trafo,\n",
        "                'Modelo': modelo,\n",
        "                'Frequência': freq,\n",
        "                'Melhores_Parâmetros': best_params,\n",
        "                'Melhor_Score_Tuning': None,\n",
        "                'Metodo_Tuning': 'KerasTuner'\n",
        "            })\n",
        "            print(f\"🎯 Melhores parâmetros encontrados (tuning Keras): {best_params}\")\n",
        "\n",
        "        # ============================\n",
        "        # ETAPA B: AVALIAÇÃO EM TIME-SERIES SPLIT (USANDO PARAMS FIXOS)\n",
        "        # ============================\n",
        "        print(\"📐 Avaliando com TimeSeriesSplit (parâmetros fixos em todos os folds)...\")\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        fold_rmse, fold_mae = [], []\n",
        "        lista_datas, lista_reais, lista_previstos = [], [], []\n",
        "\n",
        "        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X if modelo in ['LSTM','CNN','CNN_LSTM'] else X_sklearn)):\n",
        "            # % Usa índices do TimeSeriesSplit para criar conjuntos de treino/teste\n",
        "            if modelo in ['LSTM','CNN','CNN_LSTM']:\n",
        "                X_train, X_test = X_keras[train_idx], X_keras[test_idx]\n",
        "            else:\n",
        "                X_train, X_test = X_sklearn[train_idx], X_sklearn[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # & Treina com os hiperparâmetros fixos obtidos anteriormente\n",
        "            if modelo in param_grids_base.keys():\n",
        "                # % Instancia o modelo sklearn com os melhores parâmetros\n",
        "                estimator = {\n",
        "                    'SVR': SVR(),\n",
        "                    'RFR': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "                    'GBR': GradientBoostingRegressor(random_state=42),\n",
        "                    'XGB': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
        "                    'LGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
        "                }[modelo]\n",
        "\n",
        "                # & Aplica os parâmetros encontrados (best_params)\n",
        "                estimator.set_params(**best_params)\n",
        "                estimator.fit(X_train, y_train)\n",
        "                y_pred = estimator.predict(X_test)\n",
        "\n",
        "                # & Salva o modelo final (substitui para cada fold: se quiser só o final, salvar fora do loop)\n",
        "                model_path = f\"modelos/{modelo}_{trafo}_{freq}.pkl\"\n",
        "                joblib.dump(estimator, model_path)\n",
        "\n",
        "            else:\n",
        "                # % Para redes neurais, constrói um novo modelo a partir dos HPS fixos e treina por fold\n",
        "                model = build_keras_from_hps_fixed(modelo, best_params)\n",
        "                early_stop_fold = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "                model.fit(X_train, y_train,\n",
        "                          epochs=epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          validation_split=0.1,\n",
        "                          callbacks=[early_stop_fold],\n",
        "                          verbose=0)\n",
        "                y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "                # & Salva modelo keras (usamos o último fold como modelo salvo)\n",
        "                model_path = f\"modelos/{modelo}_{trafo}_{freq}.h5\"\n",
        "                model.save(model_path)\n",
        "\n",
        "            # % Calcula métricas do fold e guarda resultados para plots/resumo\n",
        "            rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            fold_rmse.append(rmse)\n",
        "            fold_mae.append(mae)\n",
        "            datas_finais = datas_janela[test_idx]\n",
        "            lista_datas.append(datas_finais)\n",
        "            lista_reais.append(y_test)\n",
        "            lista_previstos.append(y_pred)\n",
        "\n",
        "            print(f\"   Fold {fold_idx+1}: RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
        "\n",
        "        # % Gera tabela de métricas por fold e plota figuras\n",
        "        df_metricas = gerar_tabela_metricas_por_fold(trafo, modelo, fold_rmse, fold_mae)\n",
        "        print(df_metricas)\n",
        "\n",
        "        plotar_ultimo_fold(lista_datas[-1], lista_reais[-1], lista_previstos[-1], trafo, modelo, freq)\n",
        "        plotar_todos_folds(lista_datas, lista_reais, lista_previstos, trafo, modelo, eixo_label='Dia' if freq=='daily' else 'Hora')\n",
        "\n",
        "        # % Registra resultados resumidos\n",
        "        resultados.append({\n",
        "            'Trafo': trafo,\n",
        "            'Modelo': modelo,\n",
        "            'Frequência': freq,\n",
        "            'RMSE Médio': np.round(np.mean(fold_rmse), 4),\n",
        "            'MAE Médio': np.round(np.mean(fold_mae), 4),\n",
        "            'RMSE Último Fold': np.round(fold_rmse[-1], 4),\n",
        "            'MAE Último Fold': np.round(fold_mae[-1], 4),\n",
        "            'Modelo_Salvo': model_path\n",
        "        })\n",
        "\n",
        "    # % Converte listas de resultados para DataFrames e retorna\n",
        "    resultados_df = pd.DataFrame(resultados)\n",
        "    parametros_df = pd.DataFrame(parametros_finais)\n",
        "    return resultados_df, parametros_df\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÕES AUXILIARES DE RELATÓRIO / SALVAMENTO DE PARÂMETROS\n",
        "# ========================================================================\n",
        "def gerar_relatorio_parametros(resultados_df, parametros_df):\n",
        "    # % Gera um relatório simples com os melhores parâmetros (último tuning salvo)\n",
        "    relatorio_completo = []\n",
        "    for _, row in resultados_df.iterrows():\n",
        "        trafo = row['Trafo']\n",
        "        modelo = row['Modelo']\n",
        "        freq = row['Frequência']\n",
        "        params_ = parametros_df[\n",
        "            (parametros_df['Trafo'] == trafo) &\n",
        "            (parametros_df['Modelo'] == modelo) &\n",
        "            (parametros_df['Frequência'] == freq)\n",
        "        ]\n",
        "        if not params_.empty:\n",
        "            best_params = params_.iloc[-1]['Melhores_Parâmetros']\n",
        "            relatorio_completo.append({\n",
        "                'Modelo': modelo,\n",
        "                'Trafo': trafo,\n",
        "                'Frequência': freq,\n",
        "                'RMSE_Médio': row['RMSE Médio'],\n",
        "                'MAE_Médio': row['MAE Médio'],\n",
        "                'Parâmetros_Otimizados': best_params\n",
        "            })\n",
        "    return pd.DataFrame(relatorio_completo)\n",
        "\n",
        "def salvar_parametros_detalhados(parametros_df, nome_arquivo='parametros_hiperparametrizacao.csv'):\n",
        "    # % Expande dicionário de hyperparams em colunas e salva CSV\n",
        "    registros = []\n",
        "    for _, row in parametros_df.iterrows():\n",
        "        base = {\n",
        "            'Trafo': row.get('Trafo'),\n",
        "            'Modelo': row.get('Modelo'),\n",
        "            'Frequência': row.get('Frequência'),\n",
        "            'Metodo_Tuning': row.get('Metodo_Tuning'),\n",
        "            'Melhor_Score_Tuning': row.get('Melhor_Score_Tuning')\n",
        "        }\n",
        "        best = row.get('Melhores_Parâmetros', {})\n",
        "        if isinstance(best, dict):\n",
        "            for k, v in best.items():\n",
        "                base[f'Param_{k}'] = v\n",
        "        else:\n",
        "            # % Quando o tuner retorna um dict-like com tipos diferentes\n",
        "            try:\n",
        "                for k, v in dict(best).items():\n",
        "                    base[f'Param_{k}'] = v\n",
        "            except Exception:\n",
        "                base['Param_raw'] = str(best)\n",
        "        registros.append(base)\n",
        "    df_expandido = pd.DataFrame(registros)\n",
        "    os.makedirs('resultados', exist_ok=True)\n",
        "    caminho = os.path.join('resultados', nome_arquivo)\n",
        "    df_expandido.to_csv(caminho, index=False, encoding='utf-8')\n",
        "    print(f\"✅ Parâmetros detalhados salvos em: {caminho}\")\n",
        "    return df_expandido\n",
        "\n",
        "# ========================================================================\n",
        "# EXEMPLO DE USO\n",
        "# ========================================================================\n",
        "# % Substitua 'df' abaixo pelo seu DataFrame contendo colunas: ['id','datahora','S']\n",
        "# & Ex.: df = pd.read_csv('seus_dados.csv', parse_dates=['datahora'])\n",
        "#\n",
        "# % Exemplo (comentado) de como chamar:\n",
        "trafos = ['T1', 'T2']                 # & lista dos trafos que deseja treinar\n",
        "resultados, parametros = treinar_e_prever_modelo_auto_sem_data_leakage(df, trafos, 'SVR', janela=30)\n",
        "#\n",
        "# % Após rodar para cada modelo que quiser, salve/compare resultados:\n",
        "relatorio = gerar_relatorio_parametros(resultados, parametros)\n",
        "salvar_parametros_detalhados(parametros)\n",
        "#\n",
        "# % IMPORTANTE: para executar para todos os modelos em sequência (exemplo):\n",
        "modelos_a_treinar = ['SVR','RFR','GBR','XGB','LGBM','LSTM','CNN','CNN_LSTM']\n",
        "trafos = ['T1']\n",
        "resultados_geral = []\n",
        "parametros_geral = []\n",
        "for m in modelos_a_treinar:\n",
        "    res_df, param_df = treinar_e_prever_modelo_auto_sem_data_leakage(df, trafos, m, janela=30, epochs=15, batch_size=32, n_iter_search=5)\n",
        "    resultados_geral.append(res_df)\n",
        "    parametros_geral.append(param_df)\n",
        "resultados_geral_df = pd.concat(resultados_geral, ignore_index=True)\n",
        "parametros_geral_df = pd.concat(parametros_geral, ignore_index=True)\n",
        "salvar_parametros_detalhados(parametros_geral_df, nome_arquivo='parametros_todos_modelos.csv')\n",
        "#\n",
        "# % Depois de rodar, verifique 'plots/', 'modelos/' e 'resultados/' para artefatos salvos.\n",
        "# & Certifique-se de ter espaço e permissões de escrita no diretório de trabalho.\n",
        "\n",
        "# ========================================================================\n",
        "# FIM DO SCRIPT\n",
        "# ========================================================================\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

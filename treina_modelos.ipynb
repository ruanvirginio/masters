{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oJDwuasZi9i2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "url_hourly = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/transformers_dataset.csv\"\n",
        "df_hourly = pd.read_csv(url_hourly,  sep=';', encoding='latin-1')\n",
        "\n",
        "url_daily = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/daily_peak_transformers_dataset.csv\"\n",
        "df_daily = pd.read_csv(url_daily,  sep=';', encoding='latin-1')\n",
        "\n",
        "# df_count = df_daily.groupby('id').count()\n",
        "\n",
        "# df_count = df_count.sort_values('datahora').tail(41).reset_index() # esses são os trafos com mais de 97,5% de linhas preenchidas\n",
        "# trafos_escolhidos = df_count['id'].unique().tolist()\n",
        "# df_filtrado = df_daily[df_daily['id'].isin(trafos_escolhidos)]\n",
        "\n",
        "# fig_aparente = px.line(df_filtrado, x='datahora', y='S', color='id',\n",
        "#                        title='Potência Aparente ao Longo do Tempo por Transformador',\n",
        "#                        labels={'S': 'Potência Aparente (kVA)', 'Dia': 'Data'})\n",
        "\n",
        "# fig_aparente.show()\n",
        "# fig_aparente.write_html(\"Demanda ao longo do tempo - IQR.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z8xHrY-i9jH"
      },
      "source": [
        "#### Algoritmo de Aprendizado de Máquina - Pico Diário, 1 feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gw3U9O2vi9jK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Treinando XGB para T1...\n",
            "✅ Modelo salvo em: modelos/XGB_T1.pkl\n",
            "     Fold Trafo Modelo  RMSE  MAE\n",
            "0  Fold 1    T1    XGB  0.06 0.04\n",
            "1  Fold 2    T1    XGB  0.05 0.03\n",
            "2  Fold 3    T1    XGB  0.04 0.03\n",
            "3  Fold 4    T1    XGB  0.06 0.05\n",
            "4  Fold 5    T1    XGB  0.09 0.08\n",
            "\n",
            "🔹 Treinando LGBM para T1...\n",
            "✅ Modelo salvo em: modelos/LGBM_T1.pkl\n",
            "     Fold Trafo Modelo  RMSE  MAE\n",
            "0  Fold 1    T1   LGBM  0.05 0.04\n",
            "1  Fold 2    T1   LGBM  0.04 0.03\n",
            "2  Fold 3    T1   LGBM  0.04 0.03\n",
            "3  Fold 4    T1   LGBM  0.06 0.05\n",
            "4  Fold 5    T1   LGBM  0.08 0.06\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================\n",
        "# IMPORTAÇÕES E CONFIGURAÇÕES\n",
        "# ========================================================================\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'browser'\n",
        "\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
        "import joblib\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# Fixando seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÕES AUXILIARES\n",
        "# ========================================================================\n",
        "def gerar_tabela_metricas_por_fold(trafo, modelo, fold_rmse, fold_mae):\n",
        "    return pd.DataFrame({\n",
        "        'Fold': [f'Fold {i+1}' for i in range(len(fold_rmse))],\n",
        "        'Trafo': trafo,\n",
        "        'Modelo': modelo,\n",
        "        'RMSE': np.round(fold_rmse, 4),\n",
        "        'MAE': np.round(fold_mae, 4)\n",
        "    })\n",
        "\n",
        "\n",
        "def plotar_resultados(datas, y_real, y_pred, trafo, modelo):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(datas, y_real, label='Real', color='blue')\n",
        "    plt.plot(datas, y_pred, label=f'Previsto ({modelo})', linestyle='--', color='orange')\n",
        "    plt.xlabel('Data', fontsize=16)\n",
        "    plt.ylabel('Potência Aparente (kVA)', fontsize=16)\n",
        "    plt.title(f'Previsão - {modelo} ({trafo})', fontsize=18)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "    plt.savefig(f'plots/PLOT_{modelo}_{trafo}.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plotar_todos_folds(lista_datas, lista_reais, lista_previstos, trafo, modelo):\n",
        "    datas_todas = pd.to_datetime(np.concatenate(lista_datas))\n",
        "    reais_todos = np.concatenate(lista_reais)\n",
        "    previstos_todos = np.concatenate(lista_previstos)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=datas_todas, y=reais_todos, mode='lines', name='Real', line=dict(color='blue')))\n",
        "    fig.add_trace(go.Scatter(x=datas_todas, y=previstos_todos, mode='lines', name=f'Previsto ({modelo})', line=dict(color='orange', dash='dash')))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'Previsão em Todos os Folds - {trafo} ({modelo})',\n",
        "        xaxis_title='Data',\n",
        "        yaxis_title='Potência Aparente',\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÃO PRINCIPAL DE TREINAMENTO E PREVISÃO\n",
        "# ========================================================================\n",
        "def treinar_e_prever_modelo(data, trafos_escolhidos, modelo, janela, epochs=20, batch_size=32):\n",
        "    resultados = []\n",
        "    os.makedirs('modelos', exist_ok=True)\n",
        "\n",
        "    for trafo in trafos_escolhidos:\n",
        "        print(f\"\\n🔹 Treinando {modelo} para {trafo}...\")\n",
        "\n",
        "        df = data[data['id'] == trafo].copy()\n",
        "        df = df[['datahora', 'S']].set_index('datahora').sort_index()\n",
        "\n",
        "        # Cria janelas deslizantes\n",
        "        X, y = [], []\n",
        "        for i in range(janela, len(df)):\n",
        "            X.append(df.iloc[i-janela:i].values)\n",
        "            y.append(df.iloc[i, 0])\n",
        "        X, y = np.array(X), np.array(y)\n",
        "\n",
        "        # Ajuste de shape para redes neurais\n",
        "        if modelo in ['LSTM', 'CNN', 'CNN_LSTM']:\n",
        "            X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2]))\n",
        "\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        fold_rmse, fold_mae = [], []\n",
        "        lista_datas, lista_reais, lista_previstos = [], [], []\n",
        "\n",
        "        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # Inicializa modelo\n",
        "            if modelo == 'SVR':\n",
        "                regressor = SVR(kernel='rbf', C=100, gamma=0.001, epsilon=0.01)\n",
        "            elif modelo == 'RFR':\n",
        "                regressor = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
        "            elif modelo == 'GBR':\n",
        "                regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "            elif modelo == 'LGBM':\n",
        "                regressor = LGBMRegressor(n_estimators=100, random_state=42)\n",
        "            elif modelo == 'XGB':\n",
        "                regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
        "            elif modelo == 'LSTM':\n",
        "                regressor = Sequential([\n",
        "                    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "                    LSTM(50),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "                regressor.compile(optimizer='adam', loss='mse')\n",
        "            elif modelo == 'CNN':\n",
        "                regressor = Sequential([\n",
        "                    Conv1D(64, 2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "                    MaxPooling1D(2),\n",
        "                    Flatten(),\n",
        "                    Dense(50, activation='relu'),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "                regressor.compile(optimizer='adam', loss='mse')\n",
        "            elif modelo == 'CNN_LSTM':\n",
        "                regressor = Sequential([\n",
        "                    Conv1D(64, 2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "                    MaxPooling1D(2),\n",
        "                    LSTM(50, return_sequences=True),\n",
        "                    LSTM(50),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "                regressor.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "            # Treinamento\n",
        "            if modelo in ['LSTM', 'CNN', 'CNN_LSTM']:\n",
        "                regressor.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "                y_pred = regressor.predict(X_test)\n",
        "            else:\n",
        "                regressor.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "                y_pred = regressor.predict(X_test.reshape(X_test.shape[0], -1))\n",
        "\n",
        "            # Métricas\n",
        "            rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            fold_rmse.append(rmse)\n",
        "            fold_mae.append(mae)\n",
        "\n",
        "            datas_finais = df.index[test_idx]\n",
        "            lista_datas.append(datas_finais)\n",
        "            lista_reais.append(y_test)\n",
        "            lista_previstos.append(y_pred)\n",
        "\n",
        "            if fold_idx == 4:\n",
        "                plotar_resultados(datas_finais, y_test, y_pred, trafo, modelo)\n",
        "\n",
        "        # Salva modelo final\n",
        "        modelo_path = f\"modelos/{modelo}_{trafo}.{'h5' if modelo in ['LSTM', 'CNN', 'CNN_LSTM'] else 'pkl'}\"\n",
        "        if modelo in ['LSTM', 'CNN', 'CNN_LSTM']:\n",
        "            regressor.save(modelo_path)\n",
        "        else:\n",
        "            joblib.dump(regressor, modelo_path)\n",
        "        print(f\"✅ Modelo salvo em: {modelo_path}\")\n",
        "\n",
        "        df_metricas = gerar_tabela_metricas_por_fold(trafo, modelo, fold_rmse, fold_mae)\n",
        "        print(df_metricas)\n",
        "\n",
        "        plotar_todos_folds(lista_datas, lista_reais, lista_previstos, trafo, modelo)\n",
        "\n",
        "        resultados.append({\n",
        "            'Trafo': trafo,\n",
        "            'Modelo': modelo,\n",
        "            'RMSE Médio': np.round(np.mean(fold_rmse), 4),\n",
        "            'MAE Médio': np.round(np.mean(fold_mae), 4)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "# ========================================================================\n",
        "# df_daily deve estar carregado e normalizado\n",
        "# trafos = df_daily['id'].unique().tolist()\n",
        "\n",
        "trafos = ['T1'] \n",
        "# resultados_SVR = treinar_e_prever_modelo(df_daily, trafos, modelo='SVR', janela=365)\n",
        "# resultados_LSTM = treinar_e_prever_modelo(df_daily, trafos, modelo='LSTM', janela=365)\n",
        "# resultados_CNN = treinar_e_prever_modelo(df_daily, trafos, modelo='CNN', janela=365)\n",
        "# resultados_RFR = treinar_e_prever_modelo(df_daily, trafos, modelo='RFR', janela=365)\n",
        "# resultados_GBR = treinar_e_prever_modelo(df_daily, trafos, modelo='GBR', janela=365)\n",
        "resultados_XGB = treinar_e_prever_modelo(df_daily, trafos, modelo='XGB', janela=365)\n",
        "resultados_LGBM = treinar_e_prever_modelo(df_daily, trafos, modelo='LGBM', janela=365)\n",
        "# resultados_CNN_LSTM = treinar_e_prever_modelo(df_daily, trafos, modelo='CNN_LSTM', janela=365)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Modelo carregado: G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\modelos\\SVR_T1.pkl\n",
            "✅ Modelo carregado: G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\modelos\\XGB_T1.pkl\n",
            "✅ Modelo carregado: G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\modelos\\LGBM_T1.pkl\n",
            "✅ Modelo carregado: G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\modelos\\GBR_T1.pkl\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Could not deserialize 'keras.metrics.mse' because it is not a KerasSaveable subclass",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(caminho):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m modelo \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCNN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCNN_LSTM\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         modelos_carregados[(modelo, trafo)] = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     29\u001b[39m         modelos_carregados[(modelo, trafo)] = joblib.load(caminho)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib.load_model(\n\u001b[32m    190\u001b[39m         filepath,\n\u001b[32m    191\u001b[39m         custom_objects=custom_objects,\n\u001b[32m    192\u001b[39m         \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m,\n\u001b[32m    193\u001b[39m         safe_mode=safe_mode,\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith((\u001b[33m\"\u001b[39m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.hdf5\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    204\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:159\u001b[39m, in \u001b[36mload_model_from_hdf5\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    155\u001b[39m training_config = json_utils.decode(training_config)\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[32m    158\u001b[39m model.compile(\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     **\u001b[43msaving_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m )\n\u001b[32m    163\u001b[39m saving_utils.try_build_compiled_arguments(model)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:143\u001b[39m, in \u001b[36mcompile_args_from_training_config\u001b[39m\u001b[34m(training_config, custom_objects)\u001b[39m\n\u001b[32m    141\u001b[39m loss_config = training_config.get(\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     loss = \u001b[43m_deserialize_nested_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[32m    145\u001b[39m     loss = _resolve_compile_arguments_compat(loss, loss_config, losses)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:202\u001b[39m, in \u001b[36m_deserialize_nested_config\u001b[39m\u001b[34m(deserialize_fn, config)\u001b[39m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_single_object(config):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    205\u001b[39m         k: _deserialize_nested_config(deserialize_fn, v)\n\u001b[32m    206\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config.items()\n\u001b[32m    207\u001b[39m     }\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses\\__init__.py:155\u001b[39m, in \u001b[36mdeserialize\u001b[39m\u001b[34m(name, custom_objects)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.losses.deserialize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeserialize\u001b[39m(name, custom_objects=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserializes a serialized loss class/function instance.\u001b[39;00m\n\u001b[32m    145\u001b[39m \n\u001b[32m    146\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m        A Keras `Loss` instance or a loss function.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:590\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(config, custom_objects, safe_mode, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[32m    589\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m                \u001b[49m\u001b[43mserialize_with_public_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_module_name\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[32m    597\u001b[39m             serialize_with_public_class(\n\u001b[32m    598\u001b[39m                 module_objects[config], inner_config=inner_config\n\u001b[32m    599\u001b[39m             ),\n\u001b[32m    600\u001b[39m             custom_objects=custom_objects,\n\u001b[32m    601\u001b[39m         )\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:693\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(config, custom_objects, safe_mode, **kwargs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m class_name == \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    692\u001b[39m     fn_name = inner_config\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mshared_object_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruanv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:825\u001b[39m, in \u001b[36m_retrieve_class_or_fn\u001b[39m\u001b[34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[39m\n\u001b[32m    823\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    826\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not deserialize \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m because \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    827\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit is not a KerasSaveable subclass\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    828\u001b[39m         )\n\u001b[32m    829\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m    830\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    831\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m because \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be imported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    833\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    834\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Could not deserialize 'keras.metrics.mse' because it is not a KerasSaveable subclass"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "# Lista de transformadores e modelos\n",
        "trafos = ['T1', 'T2']\n",
        "modelos = ['SVR', 'XGB', 'LGBM', 'GBR', 'LSTM', 'CNN', 'CNN_LSTM']\n",
        "\n",
        "# Diretório base onde estão os modelos\n",
        "base_dir = r\"G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\modelos\"\n",
        "\n",
        "# Dicionário para armazenar os modelos carregados\n",
        "modelos_carregados = {}\n",
        "\n",
        "for trafo in trafos:\n",
        "    for modelo in modelos:\n",
        "        # Define a extensão correta\n",
        "        extensao = \"h5\" if modelo in [\"LSTM\", \"CNN\", \"CNN_LSTM\"] else \"pkl\"\n",
        "        caminho = os.path.join(base_dir, f\"{modelo}_{trafo}.{extensao}\")\n",
        "        \n",
        "        # Inicializa a chave no dicionário\n",
        "        modelos_carregados[(modelo, trafo)] = None\n",
        "        \n",
        "        # Carrega apenas se o arquivo existir\n",
        "        if os.path.exists(caminho):\n",
        "            if modelo in [\"LSTM\", \"CNN\", \"CNN_LSTM\"]:\n",
        "                modelos_carregados[(modelo, trafo)] = load_model(caminho)\n",
        "            else:\n",
        "                modelos_carregados[(modelo, trafo)] = joblib.load(caminho)\n",
        "            print(f\"✅ Modelo carregado: {caminho}\")\n",
        "        else:\n",
        "            print(f\"⚠️ Arquivo não encontrado, ignorando: {caminho}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos analisar seus dados originais para entender a estrutura\n",
        "print(\"=== ANALISANDO DADOS ORIGINAIS ===\")\n",
        "print(f\"Shape do df_daily: {df_daily.shape}\")\n",
        "print(f\"Colunas: {df_daily.columns.tolist()}\")\n",
        "print(f\"Tipos de dados:\\n{df_daily.dtypes}\")\n",
        "\n",
        "# Ver um exemplo dos dados do T1\n",
        "print(\"\\n=== DADOS DO TRAFO T1 ===\")\n",
        "df_t1 = df_daily[df_daily['id'] == 'T1'].copy()\n",
        "print(f\"Quantidade de dados T1: {len(df_t1)}\")\n",
        "print(f\"Período: {df_t1['datahora'].min()} até {df_t1['datahora'].max()}\")\n",
        "print(f\"Primeiras linhas T1:\")\n",
        "print(df_t1.head())\n",
        "\n",
        "# Ver a distribuição da variável target 'S'\n",
        "print(f\"\\nEstatísticas da Potência Aparente (S):\")\n",
        "print(df_t1['S'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diário, multi-feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def criar_features_daily(df_daily):\n",
        "    df_featured_daily = df_daily.copy()\n",
        "        \n",
        "    # Features para dados diários\n",
        "    df_featured_daily['datahora'] = pd.to_datetime(df_featured_daily['datahora'])\n",
        "    df_featured_daily = df_featured_daily.sort_values(['id', 'datahora'])\n",
        "    \n",
        "    df_featured_daily['day_of_week'] = df_featured_daily['datahora'].dt.dayofweek\n",
        "    df_featured_daily['month'] = df_featured_daily['datahora'].dt.month\n",
        "    df_featured_daily['year'] = df_featured_daily['datahora'].dt.year\n",
        "    df_featured_daily['is_weekend'] = (df_featured_daily['day_of_week'] >= 5).astype(int)\n",
        "    \n",
        "    # Features cíclicas diárias\n",
        "    df_featured_daily['sin_day_of_week'] = np.sin(2 * np.pi * df_featured_daily['day_of_week'] / 7)\n",
        "    df_featured_daily['cos_day_of_week'] = np.cos(2 * np.pi * df_featured_daily['day_of_week'] / 7)\n",
        "    df_featured_daily['sin_month'] = np.sin(2 * np.pi * df_featured_daily['month'] / 12)\n",
        "    df_featured_daily['cos_month'] = np.cos(2 * np.pi * df_featured_daily['month'] / 12)\n",
        "    \n",
        "    # Lags diários (sazonais)\n",
        "    for lag in [1, 7, 30, 365]:  # 1 dia, 1 semana, 1 mês, 1 ano\n",
        "        df_featured_daily[f'S{lag}'] = df_featured_daily.groupby('id')['S'].shift(lag)\n",
        "    \n",
        "    # Médias móveis diárias\n",
        "    for window in [7, 30, 90]:  # 1 semana, 1 mês, 3 meses\n",
        "        df_featured_daily[f'S_rolling_mean_{window}'] = df_featured_daily.groupby('id')['S'].transform(\n",
        "            lambda x: x.rolling(window, min_periods=1).mean())\n",
        "        \n",
        "    return df_featured_daily\n",
        "\n",
        "print(\"\\nCriando features de engenharia de tempo para dados diários...\")\n",
        "daily_features = criar_features_daily(df_daily)\n",
        "daily_features\n",
        "# data_com_features.dropna(inplace=True)\n",
        "print(\"✅ Features diárias criadas com sucesso.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hora em Hora, multi-feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def criar_features_hourly(df_hourly):\n",
        "    # \"\"\"Cria features específicas para cada modo\"\"\"    \n",
        "    df_featured_hourly = df_hourly.copy()\n",
        "\n",
        "    # Features para dados horários\n",
        "    df_featured_hourly['datahora'] = pd.to_datetime(df_featured_hourly['datahora'])\n",
        "    df_featured_hourly = df_featured_hourly.sort_values(['id', 'datahora'])\n",
        "    \n",
        "    df_featured_hourly['hour'] = df_featured_hourly['datahora'].dt.hour\n",
        "    df_featured_hourly['day_of_week'] = df_featured_hourly['datahora'].dt.dayofweek\n",
        "    df_featured_hourly['is_weekend'] = (df_featured_hourly['day_of_week'] >= 5).astype(int)\n",
        "    \n",
        "    # Features cíclicas horárias\n",
        "    df_featured_hourly['sin_hour'] = np.sin(2 * np.pi * df_featured_hourly['hour'] / 24)\n",
        "    df_featured_hourly['cos_hour'] = np.cos(2 * np.pi * df_featured_hourly['hour'] / 24)\n",
        "    \n",
        "    # Lags horários\n",
        "    for lag in [1, 24, 168]:  # 1h, 1 dia, 1 semana\n",
        "        df_featured_hourly[f'S_lag_{lag}'] = df_featured_hourly.groupby('id')['S'].shift(lag)\n",
        "    \n",
        "    # Médias móveis horárias\n",
        "    df_featured_hourly['S_rolling_mean_24'] = df_featured_hourly.groupby('id')['S'].transform(\n",
        "        lambda x: x.rolling(24, min_periods=1).mean())\n",
        "        \n",
        "        \n",
        "    return df_featured_hourly\n",
        "\n",
        "print(\"\\nCriando features de engenharia de tempo para dados horários...\")\n",
        "hourly_features = criar_features_hourly(df_hourly)\n",
        "\n",
        "# data_com_features.dropna(inplace=True)\n",
        "print(\"✅ Features horárias criadas com sucesso.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

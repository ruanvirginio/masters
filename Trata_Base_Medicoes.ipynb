{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97a6c4b",
   "metadata": {},
   "source": [
    "### Tratando outliers, mascarando dados sensíveis e normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5221b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "caminho_base = \"G:/Meu Drive/Estudos/Mestrado/Projeto de Mestrado/\"\n",
    "\n",
    "# Define caminhos para os dados brutos e para os dados tratados\n",
    "caminho_dados_originais = os.path.join(caminho_base, \"bases_originais\")\n",
    "caminho_dados_tratados = os.path.join(caminho_base, \"bases_tratadas\")\n",
    "\n",
    "os.makedirs(caminho_dados_tratados, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNÇÕES AUXILIARES PARA LIMPEZA DE DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "def corrigir_sequencias_de_zeros(df_grupo, coluna, min_sequencia=2):\n",
    "    # Encontra sequências de zeros (>= min_sequencia) e as preenche usando interpolação linear.\n",
    "    df = df_grupo.copy()\n",
    "    is_zero = df[coluna] == 0\n",
    "    blocos = (is_zero != is_zero.shift()).cumsum()\n",
    "    tamanho_blocos = df.groupby(blocos)[coluna].transform('size')\n",
    "    condicao = (is_zero) & (tamanho_blocos >= min_sequencia)\n",
    "    df.loc[condicao, coluna] = np.nan\n",
    "    df[coluna] = df[coluna].interpolate(method='linear')\n",
    "    df[coluna] = df[coluna].ffill().bfill()\n",
    "    return df\n",
    "\n",
    "def filtrar_e_imputar_spikes_robusto(df_grupo, coluna, threshold_pico=1.5):\n",
    "    # Corrige picos extremos e zeros isolados que sobraram.\n",
    "    df = df_grupo.copy()\n",
    "    prev_val = df[coluna].shift(1)\n",
    "    next_val = df[coluna].shift(-1)\n",
    "    media_vizinhos = (prev_val + next_val) / 2\n",
    "    condicao_pico = (df[coluna] > prev_val * threshold_pico) & (df[coluna] > next_val * threshold_pico) & (prev_val > 0) & (next_val > 0)\n",
    "    condicao_queda = (df[coluna] == 0) & (prev_val.notna()) & (prev_val != 0) & (next_val.notna()) & (next_val != 0)\n",
    "    is_outlier = condicao_pico | condicao_queda\n",
    "    df[coluna] = np.where(is_outlier, media_vizinhos, df[coluna])\n",
    "    df[coluna].fillna(df_grupo[coluna], inplace=True)\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# PIPELINE PRINCIPAL DE PROCESSAMENTO DE DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- INICIANDO PROCESSAMENTO ---\")\n",
    "\n",
    "# ETAPA 1: Carregando dados brutos\n",
    "df = pd.read_csv(os.path.join(caminho_dados_originais, \"Medicoes_2018-2024.csv\"), sep=',', encoding='latin-1', skiprows=1)\n",
    "\n",
    "# ETAPA 2: Gerando/carregando o mapeamento de transformadores\n",
    "caminho_mapeamento = os.path.join(caminho_dados_originais, \"mapeamento_trafos.json\")\n",
    "\n",
    "if not os.path.exists(caminho_mapeamento):\n",
    "    trafos_originais = df['Equipamento Medição'].unique()\n",
    "    grupos = defaultdict(list)\n",
    "    for trafo in sorted(trafos_originais):\n",
    "        sigla = trafo[:3]\n",
    "        grupos[sigla].append(trafo)\n",
    "    dicionario_mascarado = {}\n",
    "    contador_t = 1\n",
    "    alfabeto = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for sigla in sorted(grupos.keys()):\n",
    "        trafos_no_grupo = grupos[sigla]\n",
    "        if len(trafos_no_grupo) == 1:\n",
    "            dicionario_mascarado[trafos_no_grupo[0]] = f\"T{contador_t}\"\n",
    "        else:\n",
    "            for i, nome_original in enumerate(trafos_no_grupo):\n",
    "                dicionario_mascarado[nome_original] = f\"T{contador_t}{alfabeto[i]}\"\n",
    "        contador_t += 1\n",
    "    with open(caminho_mapeamento, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dicionario_mascarado, f, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    with open(caminho_mapeamento, 'r', encoding='utf-8') as f:\n",
    "        dicionario_mascarado = json.load(f)\n",
    "\n",
    "# ETAPA 3: Realizando pré-processamento inicial, ajustando tipo de dados, renomeando, etc\n",
    "df.rename(columns={'Potência Ativa': 'P', 'Potência Reativa': 'Q', 'Data/Hora Medição': 'datahora', 'Equipamento Medição': 'id'}, inplace=True)\n",
    "df['id'] = df['id'].map(dicionario_mascarado)\n",
    "df['P'] = pd.to_numeric(df['P'].str.replace(',', '.'), errors='coerce')\n",
    "df['Q'] = pd.to_numeric(df['Q'].str.replace(',', '.'), errors='coerce')\n",
    "df['datahora'] = pd.to_datetime(df['datahora'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "df.dropna(subset=['id', 'datahora', 'P', 'Q'], inplace=True)\n",
    "df['S'] = np.sqrt(df['P'].abs()**2 + df['Q'].abs()**2)\n",
    "# df = df[['id', 'datahora', 'S']]\n",
    "\n",
    "# ETAPA 4: Aplicando filtros de limpeza de outliers\n",
    "# Corrigir as sequências de zeros\n",
    "df_etapa_1 = df.groupby('id').apply(lambda group: corrigir_sequencias_de_zeros(group, 'S')).reset_index(drop=True)\n",
    "# Corrigir os picos isolados\n",
    "df_limpo = df_etapa_1.groupby('id').apply(lambda group: filtrar_e_imputar_spikes_robusto(group, 'S')).reset_index(drop=True)\n",
    "\n",
    "# ETAPA 5: Aplicando correção de escala (> 50)\n",
    "df_limpo.loc[df_limpo['S'] > 50, 'S'] /= 10\n",
    "\n",
    "# ETAPA 6: Normalizando a demanda (Min-Max scalrr)\n",
    "df_limpo['S_normalizado'] = df_limpo.groupby('id')['S'].transform(\n",
    "    lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)).flatten()\n",
    ")\n",
    "\n",
    "# ETAPA 7: Gerando e salvando o dataset final\n",
    "df_hourly = df_limpo[['id', 'datahora', 'S_normalizado']].copy()\n",
    "df_hourly.rename(columns={'S_normalizado': 'S'}, inplace=True) # Renomeia para 'S' pra simplificar\n",
    "\n",
    "caminho_saida_csv = os.path.join(caminho_dados_tratados, 'transformers_dataset.csv')\n",
    "df_hourly.to_csv(caminho_saida_csv, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e461d",
   "metadata": {},
   "source": [
    "#### Gráficos Pré e Pós tratamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"vscode\"\n",
    "\n",
    "fig_aparente_orig = px.line(df, x='datahora', y='P', color='id',\n",
    "                       title='Potência Aparente ao Longo do Tempo por Transformador',\n",
    "                       labels={'S': 'Potência Aparente (kVA)', 'Dia': 'Data'})\n",
    "\n",
    "fig_aparente_orig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767af74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_aparente_trat = px.line(df_hourly, x='datahora', y='S', color='id',\n",
    "                       title='Potência Aparente ao Longo do Tempo por Transformador - Tratado',\n",
    "                       labels={'S': 'Potência Aparente (kVA)', 'Dia': 'Data'})\n",
    "\n",
    "fig_aparente_trat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2550b3",
   "metadata": {},
   "source": [
    "### Ajustando para pegar a potência máx, min e méd por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly['datahora'] = pd.to_datetime(df_hourly['datahora'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_daily = (\n",
    "    df_hourly\n",
    "    .set_index('datahora')\n",
    "    .groupby('id')\n",
    "    .resample('D')\n",
    "    .agg(\n",
    "        Smax=('S', 'max'),\n",
    "        Smin=('S', 'min'),\n",
    "        Smean=('S', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "df_daily.to_csv(r\"G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\bases_tratadas\\daily_peak_transformers_dataset.csv\", index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9af382",
   "metadata": {},
   "source": [
    "## Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd = pd.read_csv('G:/Meu Drive/Estudos/Mestrado/Github/masters/bases_originais/EntrantesGD.csv',  sep=';', encoding='latin-1')\n",
    "# clientes = pd.read_csv('G:/Meu Drive/Estudos/Mestrado/Github/masters/bases_originais/Base_Quantidade_Clientes.csv', sep=';', encoding='latin-1')\n",
    "\n",
    "# data_inicio = '2018-01-01'\n",
    "# data_fim = '2024-12-31'\n",
    "# datas_completas = pd.date_range(start=data_inicio, end=data_fim, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd.rename(columns={\n",
    "#     'Tempo': 'datahora',\n",
    "# }, inplace=True)\n",
    "\n",
    "# gd['datahora'] = pd.to_datetime(gd['datahora'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# trafos = gd[\"TRAFO\"].unique()\n",
    "# trafo_datas = pd.DataFrame([(d, t) for d in datas_completas for t in trafos], columns=[\"datahora\", \"TRAFO\"])\n",
    "# gd_max = gd.groupby([\"datahora\", \"TRAFO\"])['PotenciaAcumulada'].max().reset_index()\n",
    "# gd = trafo_datas.merge(gd_max, on=[\"datahora\", \"TRAFO\"], how=\"left\").fillna(0)\n",
    "# gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa024227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clientes['DATA'] = pd.to_datetime(clientes['DATA'])\n",
    "\n",
    "# # One-hot encoding\n",
    "# dummies = pd.get_dummies(clientes[['CLASSE', 'DSC_GRUPO_FORNECIMENTO']].astype(str),\n",
    "#                          prefix=['classe', 'fornec'])\n",
    "\n",
    "# # Multiplicando as colunas pelo valor de QTD_CLIENTES, pra ser um \"peso\"\n",
    "# dummies_mult = dummies.multiply(clientes['QTD_CLIENTES'], axis=0)\n",
    "\n",
    "# clientes_concat = pd.concat([clientes[['TRAFO', 'DATA']], dummies_mult], axis=1)\n",
    "# clientes2 = clientes_concat.groupby(['TRAFO', 'DATA']).sum().reset_index()\n",
    "# clientes2.drop('classe_0', axis=1) # drop classe que n existe\n",
    "\n",
    "# clientes2.rename(columns={\n",
    "#     'DATA': 'datahora',\n",
    "# }, inplace=True)\n",
    "\n",
    "\n",
    "# horas = pd.DataFrame({\n",
    "#     'HORA': pd.date_range('00:00', '23:00', freq='H').time\n",
    "# })\n",
    "\n",
    "# clientes2 = clientes2.merge(horas, how='cross')\n",
    "\n",
    "# clientes2['datahora'] = pd.to_datetime(\n",
    "#     clientes2['datahora'].dt.strftime('%Y-%m-%d') + ' ' + clientes2['HORA'].astype(str)\n",
    "# )\n",
    "\n",
    "# clientes2.drop(columns='HORA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55973e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd['id'] = gd['TRAFO'].map(dicionario_mascarado)\n",
    "# clientes2['id'] = clientes2['TRAFO'].map(dicionario_mascarado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd = gd[['datahora', 'PotenciaAcumulada', 'id']]\n",
    "# clientes2 = clientes2[['id', 'datahora',\t'classe_0',\t'classe_1',\t'classe_2',\t'classe_3',\t'classe_4',\t'classe_5',\t'classe_6',\t'classe_7',\t'classe_8',\t'fornec_ALTA',\t'fornec_BAIXA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(df_hourly, gd, on=['datahora', 'id'], how='left')\n",
    "\n",
    "# df = pd.merge(df, clientes2, on=['datahora', 'id'], how='left')\n",
    "\n",
    "# data = df.fillna(0)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(r\"G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\bases_tratadas\\full_hourly_transformers_dataset.csv\", index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

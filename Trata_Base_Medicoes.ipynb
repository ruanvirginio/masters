{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97a6c4b",
   "metadata": {},
   "source": [
    "### Tratando outliers, mascarando dados sensíveis e normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5221b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO PIPELINE DE PROCESSAMENTO ---\n",
      "ETAPA 1/7: Carregando dados brutos...\n",
      "ETAPA 2/7: Gerando ou carregando o mapeamento de transformadores...\n",
      "  -> Arquivo de mapeamento encontrado. Carregando...\n",
      "ETAPA 3/7: Realizando pré-processamento inicial...\n",
      "ETAPA 4/7: Aplicando filtros de limpeza de outliers...\n",
      "ETAPA 5/7: Aplicando correção de escala (>50)...\n",
      "ETAPA 6/7: Normalizando a demanda (Min-Max)...\n",
      "ETAPA 7/7: Gerando e salvando o dataset final...\n",
      "\n",
      "--- PIPELINE CONCLUÍDO ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Configurações do pandas e warnings\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SEÇÃO 1: DEFINIÇÃO DE CAMINHOS (PORTÁTIL: COLAB/LOCAL)\n",
    "# ==============================================================================\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    caminho_base = \"/content/drive/MyDrive/Estudos/Mestrado/Projeto de Mestrado/\"\n",
    "else:\n",
    "    caminho_base = \"G:/Meu Drive/Estudos/Mestrado/Projeto de Mestrado/\"\n",
    "\n",
    "# Define caminhos para os dados brutos e para os dados tratados\n",
    "caminho_dados_originais = os.path.join(caminho_base, \"bases_originais\")\n",
    "caminho_dados_tratados = os.path.join(caminho_base, \"bases_tratadas\")\n",
    "\n",
    "# Cria a pasta de saída se ela não existir\n",
    "os.makedirs(caminho_dados_tratados, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# SEÇÃO 2: FUNÇÕES AUXILIARES PARA LIMPEZA DE DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "def corrigir_sequencias_de_zeros(df_grupo, coluna, min_sequencia=2):\n",
    "    # Encontra sequências de zeros (>= min_sequencia) e as preenche usando interpolação linear.\n",
    "    df = df_grupo.copy()\n",
    "    is_zero = df[coluna] == 0\n",
    "    blocos = (is_zero != is_zero.shift()).cumsum()\n",
    "    tamanho_blocos = df.groupby(blocos)[coluna].transform('size')\n",
    "    condicao = (is_zero) & (tamanho_blocos >= min_sequencia)\n",
    "    df.loc[condicao, coluna] = np.nan\n",
    "    df[coluna] = df[coluna].interpolate(method='linear')\n",
    "    df[coluna] = df[coluna].ffill().bfill()\n",
    "    return df\n",
    "\n",
    "def filtrar_e_imputar_spikes_robusto(df_grupo, coluna, threshold_pico=1.5):\n",
    "    # Corrige picos extremos e zeros isolados que sobraram.\n",
    "    df = df_grupo.copy()\n",
    "    prev_val = df[coluna].shift(1)\n",
    "    next_val = df[coluna].shift(-1)\n",
    "    media_vizinhos = (prev_val + next_val) / 2\n",
    "    condicao_pico = (df[coluna] > prev_val * threshold_pico) & (df[coluna] > next_val * threshold_pico) & (prev_val > 0) & (next_val > 0)\n",
    "    condicao_queda = (df[coluna] == 0) & (prev_val.notna()) & (prev_val != 0) & (next_val.notna()) & (next_val != 0)\n",
    "    is_outlier = condicao_pico | condicao_queda\n",
    "    df[coluna] = np.where(is_outlier, media_vizinhos, df[coluna])\n",
    "    df[coluna].fillna(df_grupo[coluna], inplace=True)\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# SEÇÃO 3: PIPELINE PRINCIPAL DE PROCESSAMENTO DE DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- INICIANDO PIPELINE DE PROCESSAMENTO ---\")\n",
    "\n",
    "# --- 3.1 Carregamento dos Dados Brutos ---\n",
    "print(\"ETAPA 1/7: Carregando dados brutos...\")\n",
    "try:\n",
    "    df = pd.read_csv(os.path.join(caminho_dados_originais, \"Medicoes_2018-2024.csv\"), sep=',', encoding='latin-1', skiprows=1)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: Arquivo 'Medicoes_2018-2024.csv' não encontrado no caminho especificado.\")\n",
    "    exit() # Encerra o script se o arquivo principal não for encontrado\n",
    "\n",
    "# --- 3.2 Geração e Carregamento do Dicionário de Mapeamento ---\n",
    "print(\"ETAPA 2/7: Gerando ou carregando o mapeamento de transformadores...\")\n",
    "caminho_mapeamento = os.path.join(caminho_dados_originais, \"mapeamento_trafos.json\")\n",
    "\n",
    "if not os.path.exists(caminho_mapeamento):\n",
    "    print(\"  -> Arquivo de mapeamento não encontrado. Gerando um novo...\")\n",
    "    trafos_originais = df['Equipamento Medição'].unique()\n",
    "    grupos = defaultdict(list)\n",
    "    for trafo in sorted(trafos_originais):\n",
    "        sigla = trafo[:3]\n",
    "        grupos[sigla].append(trafo)\n",
    "    dicionario_mascarado = {}\n",
    "    contador_t = 1\n",
    "    alfabeto = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for sigla in sorted(grupos.keys()):\n",
    "        trafos_no_grupo = grupos[sigla]\n",
    "        if len(trafos_no_grupo) == 1:\n",
    "            dicionario_mascarado[trafos_no_grupo[0]] = f\"T{contador_t}\"\n",
    "        else:\n",
    "            for i, nome_original in enumerate(trafos_no_grupo):\n",
    "                dicionario_mascarado[nome_original] = f\"T{contador_t}{alfabeto[i]}\"\n",
    "        contador_t += 1\n",
    "    with open(caminho_mapeamento, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dicionario_mascarado, f, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    print(\"  -> Arquivo de mapeamento encontrado. Carregando...\")\n",
    "    with open(caminho_mapeamento, 'r', encoding='utf-8') as f:\n",
    "        dicionario_mascarado = json.load(f)\n",
    "\n",
    "# --- 3.3 Pré-processamento Inicial ---\n",
    "print(\"ETAPA 3/7: Realizando pré-processamento inicial...\")\n",
    "df.rename(columns={'Potência Ativa': 'P', 'Potência Reativa': 'Q', 'Data/Hora Medição': 'datahora', 'Equipamento Medição': 'id'}, inplace=True)\n",
    "df['id'] = df['id'].map(dicionario_mascarado)\n",
    "df['P'] = pd.to_numeric(df['P'].str.replace(',', '.'), errors='coerce')\n",
    "df['Q'] = pd.to_numeric(df['Q'].str.replace(',', '.'), errors='coerce')\n",
    "df['datahora'] = pd.to_datetime(df['datahora'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "df.dropna(subset=['id', 'datahora', 'P', 'Q'], inplace=True)\n",
    "df['S'] = np.sqrt(df['P'].abs()**2 + df['Q'].abs()**2)\n",
    "df = df[['id', 'datahora', 'S']]\n",
    "\n",
    "# --- 3.4 Limpeza de Outliers ---\n",
    "print(\"ETAPA 4/7: Aplicando filtros de limpeza de outliers...\")\n",
    "# ETAPA A: Corrigir as sequências de zeros\n",
    "df_etapa_1 = df.groupby('id').apply(lambda group: corrigir_sequencias_de_zeros(group, 'S')).reset_index(drop=True)\n",
    "# ETAPA B: Corrigir os spikes isolados\n",
    "df_limpo = df_etapa_1.groupby('id').apply(lambda group: filtrar_e_imputar_spikes_robusto(group, 'S')).reset_index(drop=True)\n",
    "\n",
    "# --- 3.5 Correção Condicional de Escala ---\n",
    "print(\"ETAPA 5/7: Aplicando correção de escala (>50)...\")\n",
    "df_limpo.loc[df_limpo['S'] > 50, 'S'] /= 10\n",
    "\n",
    "# --- 3.6 Normalização Min-Max por Transformador ---\n",
    "print(\"ETAPA 6/7: Normalizando a demanda (Min-Max)...\")\n",
    "df_limpo['S_normalizado'] = df_limpo.groupby('id')['S'].transform(\n",
    "    lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)).flatten()\n",
    ")\n",
    "\n",
    "# --- 3.7 Preparação do DataFrame Final e Salvamento ---\n",
    "print(\"ETAPA 7/7: Gerando e salvando o dataset final...\")\n",
    "df_hourly = df_limpo[['id', 'datahora', 'S_normalizado']].copy()\n",
    "df_hourly.rename(columns={'S_normalizado': 'S'}, inplace=True) # Renomeia para 'S' para simplicidade\n",
    "\n",
    "caminho_saida_csv = os.path.join(caminho_dados_tratados, 'transformers_dataset.csv')\n",
    "df_hourly.to_csv(caminho_saida_csv, index=False, sep=';')\n",
    "\n",
    "print(f\"\\n--- PIPELINE CONCLUÍDO ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2550b3",
   "metadata": {},
   "source": [
    "### Ajustando para pegar a potência pico por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6367abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly['datahora'] = pd.to_datetime(df_hourly['datahora'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_daily = df_hourly.groupby('id', group_keys=False).apply(lambda x: x.set_index('datahora').resample('D').max()).reset_index()\n",
    "df_daily.to_csv(r\"G:\\Meu Drive\\Estudos\\Mestrado\\Github\\masters\\bases_tratadas\\daily_peak_transformers_dataset.csv\", index=False, sep=';')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TREINAMENTO ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# url_hourly = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/transformers_dataset.csv\"\n",
        "# df_hourly = pd.read_csv(url_hourly,  sep=';', encoding='latin-1')\n",
        "\n",
        "url_daily = \"https://media.githubusercontent.com/media/ruanvirginio/masters/refs/heads/main/bases_tratadas/daily_peak_transformers_dataset.csv\"\n",
        "df_daily = pd.read_csv(url_daily,  sep=';', encoding='latin-1')\n",
        "\n",
        "df = df_daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'vscode'\n",
        "\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import joblib\n",
        "\n",
        "from sklearn.base import clone  \n",
        "\n",
        "# keras_tuner para otimização das redes (usada uma vez por trafo/modelo)\n",
        "import tensorflow as tf\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(16)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras_tuner as kt\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # & para evitar backend interativo em servidores\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÕES AUXILIARES (PLOTS, MÉTRICAS, EXPORTAÇÃO)\n",
        "# ========================================================================\n",
        "def gerar_tabela_metricas_por_fold(trafo, modelo, fold_rmse, fold_mae):\n",
        "    # Gera um DataFrame com RMSE/MAE de cada fold para exibir e salvar\n",
        "    return pd.DataFrame({\n",
        "        'Fold': [f'Fold {i+1}' for i in range(len(fold_rmse))],\n",
        "        'Trafo': trafo,\n",
        "        'Modelo': modelo,\n",
        "        'RMSE': np.round(fold_rmse, 4),\n",
        "        'MAE': np.round(fold_mae, 4)\n",
        "    })\n",
        "\n",
        "def plotar_ultimo_fold(datas, y_real, y_pred, trafo, modelo, freq):\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(datas, y_real, label='Real', color='blue')\n",
        "    plt.plot(datas, y_pred, label=f'Predicted ({modelo})', linestyle='--', color='orange')\n",
        "    plt.xlabel('Day' if freq=='daily' else 'Hour')\n",
        "    plt.ylabel('Apparent Power (kVA)')\n",
        "    plt.title(f'Last Fold Prediction - {modelo} ({trafo})')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "    plt.savefig(f'plots/PLOT_{modelo}_{trafo}_{freq}_ultimo_fold.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plotar_todos_folds(lista_datas, lista_reais, lista_previstos, trafo, modelo, eixo_label='Data'):\n",
        "    # Plota todos os folds concatenados em uma figura interativa (Plotly)\n",
        "    if len(lista_datas) == 0:\n",
        "        return\n",
        "    datas_todas = pd.to_datetime(np.concatenate(lista_datas))\n",
        "    reais_todos = np.concatenate(lista_reais)\n",
        "    previstos_todos = np.concatenate(lista_previstos)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=datas_todas, y=reais_todos, mode='lines', name='Real'))\n",
        "    fig.add_trace(go.Scatter(x=datas_todas, y=previstos_todos, mode='lines', name=f'Previsto ({modelo})', line=dict(dash='dash')))\n",
        "    fig.update_layout(title=f'Previsão em Todos os Folds - {trafo} ({modelo})',\n",
        "                      xaxis_title=eixo_label, yaxis_title='Potência Aparente', hovermode='x unified')\n",
        "    fig.show()\n",
        "\n",
        "# ========================================================================\n",
        "# GRIDS BASE PARA HYPERPARAMS\n",
        "# ========================================================================\n",
        "param_grids_base = {\n",
        "    'SVR': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "        'epsilon': [0.01, 0.1, 0.5]\n",
        "    },\n",
        "    'RFR': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [5, 10, 15],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'GBR': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [3, 5]\n",
        "    },\n",
        "    'XGB': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [3, 5]\n",
        "    },\n",
        "    'LGBM': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_param_grids_por_freq(freq, modelo):\n",
        "    # Ajusta o grid padrão de acordo com a granularidade dos dados, com reduções para dados horários\n",
        "    base_params = param_grids_base[modelo].copy()\n",
        "    if freq == 'hourly':\n",
        "        if modelo == 'SVR':\n",
        "            base_params['C'] = [1, 10]\n",
        "            base_params['gamma'] = ['scale', 0.01]\n",
        "            base_params['epsilon'] = [0.1]\n",
        "        elif modelo in ['RFR', 'GBR', 'XGB', 'LGBM']:\n",
        "            base_params['n_estimators'] = [50, 100]\n",
        "    return base_params\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÃO AUXILIAR: CRIA MODELO KERAS A PARTIR DE HYPERPARAMS FIXOS\n",
        "# ========================================================================\n",
        "def build_keras_from_hps_fixed(modelo, hps):\n",
        "    # Recebe o nome do modelo (LSTM/CNN/CNN_LSTM) e um dict-like de hyperparams. Retorna um modelo Keras já compilado pronto para treinar\n",
        "    model = Sequential()\n",
        "    if modelo == 'LSTM':\n",
        "        # % Para LSTM pegamos 'units1' (número de neurônios) do best_hps\n",
        "        model.add(LSTM(units=int(hps.get('units1', 20)), input_shape=(None, 1)))\n",
        "        model.add(Dense(1))\n",
        "    elif modelo == 'CNN':\n",
        "        # % CNN simples 1D: filters e kernel são fixos nos valores do tuner\n",
        "        model.add(Conv1D(filters=int(hps.get('filters', 32)), kernel_size=2, activation='relu', input_shape=(None, 1)))\n",
        "        model.add(MaxPooling1D(2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "    elif modelo == 'CNN_LSTM':\n",
        "        # % CNN + LSTM: primeiro convolucional, depois LSTM\n",
        "        model.add(Conv1D(filters=int(hps.get('filters', 32)), kernel_size=2, activation='relu', input_shape=(None, 1)))\n",
        "        model.add(MaxPooling1D(2))\n",
        "        model.add(LSTM(units=int(hps.get('units1', 20))))\n",
        "        model.add(Dense(1))\n",
        "    else:\n",
        "        raise ValueError(f\"Modelo Keras desconhecido: {modelo}\")\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÃO PRINCIPAL: OTIMIZAÇÃO UMA-VEZ + TREINO POR FOLD COM PARAMS FIXOS\n",
        "# ========================================================================\n",
        "def treinar_e_prever_modelo_auto_sem_data_leakage(\n",
        "    data, trafos_escolhidos, modelo, janela=None, epochs=15, batch_size=32, n_iter_search=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Após detectar frequência desejada, faz para cada trafo:\n",
        "       1) Prepara janelas (sliding window)\n",
        "       2) Executa busca de hiperparâmetros no conjunto inicial (tuning set)\n",
        "       3) Usa os hiperparâmetros encontrados para treinar/avaliar cada fold (TimeSeriesSplit)\n",
        "       4) Salva modelos, métricas e parâmetros\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "    parametros_finais = []\n",
        "    os.makedirs('modelos', exist_ok=True)\n",
        "    data = data.copy()\n",
        "    data['datahora'] = pd.to_datetime(data['datahora'])\n",
        "\n",
        "    # Detecta granularidade pela mediana do delta entre timestamps\n",
        "    delta = data['datahora'].diff().median()\n",
        "    freq = 'daily' if delta >= pd.Timedelta('1D') else 'hourly'\n",
        "    if janela is None:\n",
        "        janela = 30 if freq == 'daily' else 24*3\n",
        "\n",
        "    # Percorre cada trafo solicitado\n",
        "    for trafo in trafos_escolhidos:\n",
        "        print(f\"\\n Treinando {modelo} para {trafo} ({freq})...\")\n",
        "\n",
        "        # Prepara os dados do trafo (ordenados e sem NA)\n",
        "        df_trafo = data[data['id'] == trafo][['datahora', 'S']].sort_values('datahora').dropna()\n",
        "        S_values = df_trafo['S'].values\n",
        "        datas = df_trafo['datahora'].values\n",
        "\n",
        "        n_samples = len(S_values) - janela\n",
        "        if n_samples <= 0:\n",
        "            print(f\"Dados insuficientes para {trafo}. Pulando...\")\n",
        "            continue\n",
        "\n",
        "        # % Cria janelas deslizantes (sliding window). Para X: cada amostra é uma janela de tamanho 'janela'; y é o valor seguinte\n",
        "        X = np.lib.stride_tricks.sliding_window_view(S_values, janela)[:n_samples]\n",
        "        y = S_values[janela:janela + n_samples]\n",
        "        datas_janela = datas[janela:janela + n_samples]\n",
        "\n",
        "        # % Ajuste de forma para modelos sklearn vs Keras\n",
        "        if modelo in ['LSTM', 'CNN', 'CNN_LSTM']:\n",
        "            # shape = (n_samples, janela, 1)\n",
        "            X_keras = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "        else:\n",
        "            # shape = (n_samples, janela)\n",
        "            X_sklearn = X.reshape((X.shape[0], -1))\n",
        "\n",
        "        # ============================\n",
        "        # ETAPA 1: HYPERPARAM SEARCH (1x)\n",
        "        # ============================\n",
        "        if modelo in param_grids_base.keys():\n",
        "            # Modelos scikit-learn: RandomizedSearchCV executado apenas uma vez\n",
        "            print(\"Executando RandomizedSearchCV apenas uma única vez (sem leakage)...\")\n",
        "            param_grid_freq = get_param_grids_por_freq(freq, modelo)\n",
        "\n",
        "            base_model = {\n",
        "                'SVR': SVR(),\n",
        "                'RFR': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "                'GBR': GradientBoostingRegressor(random_state=42),\n",
        "                'XGB': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
        "                'LGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
        "            }[modelo]\n",
        "\n",
        "\n",
        "            # Define um conjunto inicial (tuning set) para busca de hiperparâmetros. Usar os primeiros 70% dos exemplos evita usar informações do futuro\n",
        "            split_point = int(0.7 * len(X_sklearn))\n",
        "            X_tune, y_tune = X_sklearn[:split_point], y[:split_point]\n",
        "\n",
        "            # RandomizedSearchCV com TimeSeriesSplit interno\n",
        "            search = RandomizedSearchCV(\n",
        "                base_model,\n",
        "                param_distributions=param_grid_freq,\n",
        "                n_iter=n_iter_search,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                cv=TimeSeriesSplit(n_splits=3),\n",
        "                n_jobs=-1 if modelo != 'SVR' else 1,\n",
        "                random_state=42,\n",
        "                verbose=0\n",
        "            )\n",
        "            search.fit(X_tune, y_tune)\n",
        "            best_params = search.best_params_\n",
        "            best_score = search.best_score_\n",
        "            print(f\"Melhores parâmetros encontrados (tuning): {best_params}\")\n",
        "            # Registro do que foi encontrado\n",
        "            parametros_finais.append({\n",
        "                'Trafo': trafo,\n",
        "                'Modelo': modelo,\n",
        "                'Frequência': freq,\n",
        "                'Melhores_Parâmetros': best_params,\n",
        "                'Melhor_Score_Tuning': best_score,\n",
        "                'Metodo_Tuning': 'RandomizedSearchCV'\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            # Redes neurais: usamos KerasTuner apenas uma vez no tuning set\n",
        "            print(\"Otimizando rede neural apenas uma vez com KerasTuner (sem leakage)...\")\n",
        "\n",
        "            # & Criador de modelo para o tuner (recebe hp)\n",
        "            def build_nn_tuner(hp):\n",
        "                model = Sequential()\n",
        "                if modelo == 'LSTM':\n",
        "                    units1 = hp.Int('units1', 20, 80, step=20)\n",
        "                    model.add(LSTM(units=units1, input_shape=(X_keras.shape[1], 1)))\n",
        "                    model.add(Dense(1))\n",
        "                elif modelo == 'CNN':\n",
        "                    filters = hp.Int('filters', 32, 64, step=32)\n",
        "                    model.add(Conv1D(filters=filters, kernel_size=2, activation='relu', input_shape=(X_keras.shape[1], 1)))\n",
        "                    model.add(MaxPooling1D(2))\n",
        "                    model.add(Flatten())\n",
        "                    model.add(Dense(1))\n",
        "                elif modelo == 'CNN_LSTM':\n",
        "                    filters = hp.Int('filters', 32, 64, step=32)\n",
        "                    units1 = hp.Int('units1', 20, 60, step=20)\n",
        "                    model.add(Conv1D(filters=filters, kernel_size=2, activation='relu', input_shape=(X_keras.shape[1], 1)))\n",
        "                    model.add(MaxPooling1D(2))\n",
        "                    model.add(LSTM(units=units1))\n",
        "                    model.add(Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mse')\n",
        "                return model\n",
        "\n",
        "            # Prepara tuning set (70% inicial)\n",
        "            split_point = int(0.7 * len(X_keras))\n",
        "            X_tune, y_tune = X_keras[:split_point], y[:split_point]\n",
        "\n",
        "            # Configura o tuner (Random search, poucas trials para economizar tempo)\n",
        "            tuner = kt.RandomSearch(\n",
        "                build_nn_tuner,\n",
        "                objective='val_loss',\n",
        "                max_trials=5,           # ajuste: 5 para balancear custo/benefício\n",
        "                executions_per_trial=1,\n",
        "                overwrite=True,\n",
        "                directory='keras_tuner',\n",
        "                project_name=f'{modelo}_{trafo}_{freq}'\n",
        "            )\n",
        "\n",
        "            # Early stopping usado na busca para acelerar e evitar overfitting\n",
        "            early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "            tuner.search(X_tune, y_tune,\n",
        "                         epochs=max(5, epochs//2),    # usa menos epochs no tuner\n",
        "                         batch_size=batch_size,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[early_stop],\n",
        "                         verbose=0)\n",
        "\n",
        "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "            # Convertendo os hyperparams do tuner para um dict plano para salvar\n",
        "            best_params = best_hps.values\n",
        "            parametros_finais.append({\n",
        "                'Trafo': trafo,\n",
        "                'Modelo': modelo,\n",
        "                'Frequência': freq,\n",
        "                'Melhores_Parâmetros': best_params,\n",
        "                'Melhor_Score_Tuning': None,\n",
        "                'Metodo_Tuning': 'KerasTuner'\n",
        "            })\n",
        "            print(f\"Melhores parâmetros encontrados (tuning Keras): {best_params}\")\n",
        "\n",
        "        # ============================\n",
        "        # ETAPA 2: AVALIAÇÃO EM TIME-SERIES SPLIT (USANDO PARAMS FIXOS)\n",
        "        # ============================\n",
        "        print(\"Avaliando com TimeSeriesSplit (parâmetros fixos em todos os folds)...\")\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        fold_rmse, fold_mae = [], []\n",
        "        lista_datas, lista_reais, lista_previstos = [], [], []\n",
        "\n",
        "        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X if modelo in ['LSTM','CNN','CNN_LSTM'] else X_sklearn)):\n",
        "            # Usa índices do TimeSeriesSplit para criar conjuntos de treino/teste\n",
        "            if modelo in ['LSTM','CNN','CNN_LSTM']:\n",
        "                X_train, X_test = X_keras[train_idx], X_keras[test_idx]\n",
        "            else:\n",
        "                X_train, X_test = X_sklearn[train_idx], X_sklearn[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            # Treina com os hiperparâmetros fixos obtidos anteriormente\n",
        "            if modelo in param_grids_base.keys():\n",
        "                # Instancia o modelo sklearn com os melhores parâmetros\n",
        "                estimator = {\n",
        "                    'SVR': SVR(),\n",
        "                    'RFR': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "                    'GBR': GradientBoostingRegressor(random_state=42),\n",
        "                    'XGB': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
        "                    'LGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
        "                }[modelo]\n",
        "\n",
        "                # Aplica os parâmetros encontrados (best_params)\n",
        "                estimator.set_params(**best_params)\n",
        "                estimator.fit(X_train, y_train)\n",
        "                y_pred = estimator.predict(X_test)\n",
        "\n",
        "                # Salva o modelo final (substitui para cada fold: se quiser só o final, salvar fora do loop)\n",
        "                model_path = f\"modelos/{modelo}_{trafo}_{freq}.pkl\"\n",
        "                joblib.dump(estimator, model_path)\n",
        "\n",
        "            else:\n",
        "                # Para redes neurais, constrói um novo modelo a partir dos HPS fixos e treina por fold\n",
        "                model = build_keras_from_hps_fixed(modelo, best_params)\n",
        "                early_stop_fold = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "                model.fit(X_train, y_train,\n",
        "                          epochs=epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          validation_split=0.1,\n",
        "                          callbacks=[early_stop_fold],\n",
        "                          verbose=0)\n",
        "                y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "                # Salva modelo keras (usamos o último fold como modelo salvo)\n",
        "                model_path = f\"modelos/{modelo}_{trafo}_{freq}.h5\"\n",
        "                model.save(model_path)\n",
        "\n",
        "            # Calcula métricas do fold e guarda resultados para plots/resumo\n",
        "            rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            fold_rmse.append(rmse)\n",
        "            fold_mae.append(mae)\n",
        "            datas_finais = datas_janela[test_idx]\n",
        "            lista_datas.append(datas_finais)\n",
        "            lista_reais.append(y_test)\n",
        "            lista_previstos.append(y_pred)\n",
        "\n",
        "            print(f\"   Fold {fold_idx+1}: RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
        "\n",
        "        df_metricas = gerar_tabela_metricas_por_fold(trafo, modelo, fold_rmse, fold_mae)\n",
        "        print(df_metricas)\n",
        "\n",
        "        plotar_ultimo_fold(lista_datas[-1], lista_reais[-1], lista_previstos[-1], trafo, modelo, freq)\n",
        "        plotar_todos_folds(lista_datas, lista_reais, lista_previstos, trafo, modelo, eixo_label='Dia' if freq=='daily' else 'Hora')\n",
        "\n",
        "        resultados.append({\n",
        "            'Trafo': trafo,\n",
        "            'Modelo': modelo,\n",
        "            'Frequência': freq,\n",
        "            'RMSE Médio': np.round(np.mean(fold_rmse), 4),\n",
        "            'MAE Médio': np.round(np.mean(fold_mae), 4),\n",
        "            'RMSE Último Fold': np.round(fold_rmse[-1], 4),\n",
        "            'MAE Último Fold': np.round(fold_mae[-1], 4),\n",
        "            'Modelo_Salvo': model_path\n",
        "        })\n",
        "\n",
        "    resultados_df = pd.DataFrame(resultados)\n",
        "    parametros_df = pd.DataFrame(parametros_finais)\n",
        "    return resultados_df, parametros_df\n",
        "\n",
        "# ========================================================================\n",
        "# FUNÇÕES AUXILIARES DE RELATÓRIO / SALVAMENTO DE PARÂMETROS\n",
        "# ========================================================================\n",
        "def gerar_relatorio_parametros(resultados_df, parametros_df):\n",
        "    # % Gera um relatório simples com os melhores parâmetros (último tuning salvo)\n",
        "    relatorio_completo = []\n",
        "    for _, row in resultados_df.iterrows():\n",
        "        trafo = row['Trafo']\n",
        "        modelo = row['Modelo']\n",
        "        freq = row['Frequência']\n",
        "        params_ = parametros_df[\n",
        "            (parametros_df['Trafo'] == trafo) &\n",
        "            (parametros_df['Modelo'] == modelo) &\n",
        "            (parametros_df['Frequência'] == freq)\n",
        "        ]\n",
        "        if not params_.empty:\n",
        "            best_params = params_.iloc[-1]['Melhores_Parâmetros']\n",
        "            relatorio_completo.append({\n",
        "                'Modelo': modelo,\n",
        "                'Trafo': trafo,\n",
        "                'Frequência': freq,\n",
        "                'RMSE_Médio': row['RMSE Médio'],\n",
        "                'MAE_Médio': row['MAE Médio'],\n",
        "                'Parâmetros_Otimizados': best_params\n",
        "            })\n",
        "    return pd.DataFrame(relatorio_completo)\n",
        "\n",
        "def salvar_parametros_detalhados(parametros_df, nome_arquivo='parametros_hiperparametrizacao.csv'):\n",
        "    # % Expande dicionário de hyperparams em colunas e salva CSV\n",
        "    registros = []\n",
        "    for _, row in parametros_df.iterrows():\n",
        "        base = {\n",
        "            'Trafo': row.get('Trafo'),\n",
        "            'Modelo': row.get('Modelo'),\n",
        "            'Frequência': row.get('Frequência'),\n",
        "            'Metodo_Tuning': row.get('Metodo_Tuning'),\n",
        "            'Melhor_Score_Tuning': row.get('Melhor_Score_Tuning')\n",
        "        }\n",
        "        best = row.get('Melhores_Parâmetros', {})\n",
        "        if isinstance(best, dict):\n",
        "            for k, v in best.items():\n",
        "                base[f'Param_{k}'] = v\n",
        "        else:\n",
        "            # % Quando o tuner retorna um dict-like com tipos diferentes\n",
        "            try:\n",
        "                for k, v in dict(best).items():\n",
        "                    base[f'Param_{k}'] = v\n",
        "            except Exception:\n",
        "                base['Param_raw'] = str(best)\n",
        "        registros.append(base)\n",
        "    df_expandido = pd.DataFrame(registros)\n",
        "    os.makedirs('resultados', exist_ok=True)\n",
        "    caminho = os.path.join('resultados', nome_arquivo)\n",
        "    df_expandido.to_csv(caminho, index=False, encoding='utf-8')\n",
        "    print(f\"Parâmetros detalhados salvos em: {caminho}\")\n",
        "    return df_expandido\n",
        "\n",
        "# ========================================================================\n",
        "# ========================================================================\n",
        "# ========================================================================\n",
        "\n",
        "trafos = ['T21a', 'T40', 'T66', 'T4', 'T6', 'T10', 'T51', 'T57', 'T71']\n",
        "modelos_a_treinar = ['SVR','RFR','GBR','LGBM','LSTM']\n",
        "resultados_geral = []\n",
        "parametros_geral = []\n",
        "\n",
        "for m in modelos_a_treinar:\n",
        "    res_df, param_df = treinar_e_prever_modelo_auto_sem_data_leakage(df_daily, trafos, m, janela=30, epochs=15, batch_size=32, n_iter_search=5)\n",
        "    resultados_geral.append(res_df)\n",
        "    parametros_geral.append(param_df)\n",
        "resultados_geral_df = pd.concat(resultados_geral, ignore_index=True)\n",
        "parametros_geral_df = pd.concat(parametros_geral, ignore_index=True)\n",
        "salvar_parametros_detalhados(parametros_geral_df, nome_arquivo='parametros_todos_modelos.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
